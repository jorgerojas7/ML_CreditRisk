# ML Credit Risk Analysis

> Predicci√≥n de riesgo crediticio con pipelines reproducibles, importancia de variables y flujo de scoring/visualizaci√≥n. Incluye API/Frontend opcional para demo.

## üì¶ Componentes principales

- `ml_creditrisk/` (paquete Python):
    - `feature_grouping.py`: utilidades para agrupar variables y auditar Missing%.
    - `preprocessing.py`: ColumnTransformer desde grupos (imputaci√≥n, winsor, OHE, target-encoding) y discretizador robusto por cuantiles.
    - `importance.py`: entrenamiento XGBoost + agregado de importancias a variables originales, filtrado por umbral y plotting.
    - `models.py`: modelos base (RF, XGBoost, LightGBM, CatBoost) + evaluador y pipeline ‚ÄúGB leaves ‚Üí OneHot ‚Üí LR‚Äù.
- `notebooks/02_Feature_Engineering_Modelado.ipynb`: orquesta el flujo E2E (carga ‚Üí grupos ‚Üí preprocesamiento ‚Üí importancia ‚Üí modelos ‚Üí predicciones ‚Üí gr√°ficos).
- `api/` y `frontend/`: demo opcional con FastAPI/Streamlit (no requerida para el notebook).

## üß∞ Requisitos y entorno

- Python 3.10 recomendado (Windows soportado)
- Instalar dependencias:

```powershell
python -m venv venv
venv\Scripts\Activate.ps1
pip install -r requirements.txt
pip install -e .
```

Notas:
- Para leer el archivo .XLS del dataset se fija xlrd==1.2.0 (las versiones ‚â•2.0 ya no soportan .xls).
- LightGBM y CatBoost est√°n incluidos en requirements y se usan si est√°n instalados.

## üóÇÔ∏è Datos

- Ubicaci√≥n: `data/raw/`
- Archivos esperados:
    - `PAKDD2010_VariablesList.XLS` (nombres de columnas)
    - `PAKDD2010_Modeling_Data.txt` (modelado)
    - `PAKDD2010_Prediction_Data.txt` (scoring)

## üß™ Uso del notebook principal

1) Abrir `notebooks/02_Feature_Engineering_Modelado.ipynb` y ejecutar en orden:
     - Celda 1: carga de datos.
     - Celda 2: agrupaci√≥n de variables, exclusiones y DataFrame FINAL (auditable).
     - Celda 3: construcci√≥n del preprocesador y resumen de columnas generadas.
     - Celda 4: importancia con XGBoost; umbral configurable; crea `preprocessor_filtered` con variables ‚â• umbral (por defecto 0.02 en el cuaderno; se puede ajustar).
     - Celda 5: entrenamiento y evaluaci√≥n de modelos activos (RF, XGBoost, LightGBM, CatBoost).
     - Celda 6: predicciones sobre `Prediction_Data.txt` y columnas `score_*` en `df_pred`.
     - Celda 7: histogramas de scores por modelo.

2) B√∫squeda de hiperpar√°metros (RandomizedSearchCV):
     - La celda de HPO incluye un flag `HPO_ENABLED = False` para evitar ejecuciones largas. Cambiar a `True` para activar.
     - Los mejores pipelines quedan en `tuned_models`.
     - En la celda de predicciones, `USE_TUNED_MODELS = False` por defecto. Cambiar a `True` para usar `tuned_models` si existen.

## ü§ñ Modelos incluidos

- Random Forest (scikit-learn)
- XGBoost (xgboost)
- LightGBM (lightgbm) ‚Äì opcional si instalado
- CatBoost (catboost) ‚Äì opcional si instalado
- (Opcional) GB leaves ‚Üí OneHot ‚Üí LR (√∫til para calibraci√≥n y capturar interacciones de √°rboles)

## üß© Dise√±o del preprocesamiento

- Num√©ricas reales: imputaci√≥n (‚àí1) ‚Üí winsor (cuantiles) ‚Üí robust scaler
- Num√©ricas con prioridad (AGE, MONTHS_IN_THE_JOB): discretizaci√≥n por cuantiles (robusta)
- Categ√≥ricas baja cardinalidad y binarias: OneHotEncoder
- Categ√≥ricas alta cardinalidad: TargetEncoder (smoothing=0.3)
- Todas las decisiones dependen de `df_groups_final` como ‚Äúsingle source of truth‚Äù.

## Importancia y filtrado

- Importancias por feature output se agregan a variables raw originales.
- Se construye `preprocessor_filtered` con variables ‚â• umbral.
- Tabla de variables eliminadas incluida para auditor√≠a.

## ‚ñ∂Ô∏è API / Frontend (opcional)

Para demo r√°pida (cuando quieras mostrar un servicio):

```powershell
uvicorn api.main:app --reload --port 8000
# Docs: http://localhost:8000/docs

streamlit run frontend/streamlit_app.py --server.port 8501
# App: http://localhost:8501
```

Autenticaci√≥n (demo):
- Usuarios v√°lidos: `admin/admin123` y `analyst/analyst456`.
- Si `USE_BACKEND=false`, el login se valida localmente en el frontend (modo simulado).
- Si `USE_BACKEND=true`, el frontend llama a `POST /login` en la API y guarda un `access_token` de sesi√≥n (sin autorizaci√≥n estricta para esta demo).

Esquema de entrada del modelo (`POST /predict`):
- Campos requeridos del JSON:
    - `income` (float)
    - `age` (int)
    - `credit_amount` (float)
    - `employment_length` (int, en a√±os)
    - `debt_ratio` (float, 0‚Äì1)

La UI de Streamlit mapea autom√°ticamente el formulario de ‚ÄúCredit Application Form (Manual Input)‚Äù a estos 5 campos:
- `income` = `PERSONAL_MONTHLY_INCOME` + `OTHER_INCOMES`
- `age` = `AGE`
- `employment_length` = floor(`MONTHS_IN_THE_JOB` / 12)
- `credit_amount` ‚âà 20% de `PERSONAL_ASSETS_VALUE` (si falta, usa 10000)
- `debt_ratio` ‚âà `credit_amount` / (`income`*12 + `PERSONAL_ASSETS_VALUE`) recortado a [0, 0.9]

## üê≥ Ejecutar con Docker Compose

Requisitos: Docker Desktop y Docker Compose.

1) Construir y levantar servicios (API + Frontend):
```powershell
docker compose up --build
```

2) URLs:
- Frontend: http://localhost:8501
- FastAPI: http://localhost:8000
- Docs API: http://localhost:8000/docs

Notas:
- El frontend se conecta a la API v√≠a `API_BASE_URL` (definido en docker-compose como `http://api:8000`).
- Los vol√∫menes montan `./models` y `./data` dentro de los contenedores (`/app/models`, `/app/data`).
- Healthchecks validan que cada servicio est√© listo antes de exponerlo.

Archivos de datos auxiliares (opcional):
- La UI puede cargar un cat√°logo de ciudades de Brasil desde `data/raw/cities.csv`. Rutas soportadas autom√°ticamente:
    - `./data/raw/cities.csv` (host)
    - `/app/data/raw/cities.csv` (contenedor)
    - o define `CITIES_CSV_PATH` con la ruta al CSV
- Si el archivo no existe, la UI hace fallback: Estados por sigla fija y ciudades como texto libre (no falla).

### Variables de entorno √∫tiles
- API (servicio `api`):
    - `MODEL_PATH`: ruta al artefacto del modelo o pipeline (por ejemplo, `/app/models/pipeline.joblib`).
    - `PREPROCESSOR_PATH`: ruta al preprocesador si el modelo no lo incluye.
    - `API_HOST`, `API_PORT`, `API_DEBUG` (ya preconfigurados para Docker).
- Frontend (servicio `frontend`):
    - `API_BASE_URL`: URL de la API dentro de la red de Docker (`http://api:8000`).
    - `USE_BACKEND`: `true` para consultar la API real.
    - `CITIES_CSV_PATH`: ruta al CSV de ciudades (opcional; si no existe, hay fallback seguro).

Puedes a√±adir estas variables bajo `environment:` en `docker-compose.yml` o usar un archivo `.env`.

### Endpoints principales de la API
- `POST /login` ‚Üí autenticaci√≥n demo (devuelve `access_token` si usuario/clave v√°lidos).
- `POST /predict` ‚Üí scoring individual con el esquema de 5 campos indicado arriba.
- `POST /predict/batch` ‚Üí scoring por lote (`{"profiles": [ ... ]}`).
- `POST /simulate` ‚Üí simulaci√≥n de decisiones; par√°metros:
    - `profiles`: lista de perfiles con al menos `credit_amount` si quieres m√©tricas monetarias
    - `decision_threshold` (float, default 0.5): aprueba cuando `risk_score <= threshold`
    - `profit_margin` (float, default 0.05)
- `GET /model/info` y `GET /health` ‚Üí info b√°sica y healthcheck.

### Cambiar al modelo real
1. Copia tu artefacto entrenado a `./models` (por ejemplo `./models/pipeline_real.joblib`).
2. Edita `docker-compose.yml` ‚Üí `MODEL_PATH=/app/models/pipeline_real.joblib` (y opcional `PREPROCESSOR_PATH` si usas artefactos separados).
3. Reconstruye y levanta:
     ```powershell
     docker compose up -d --build
     ```
4. Valida `/health`, `/model/info` y una predicci√≥n simple.

## ‚úÖ Checklist de reproducibilidad

- [x] requirements.txt actualizado (incluye xlrd==1.2.0, sklearn, xgboost, lightgbm, catboost, scipy, etc.)
- [x] Paquete `ml_creditrisk` con docstrings y funciones reutilizables
- [x] Notebook principal orquestando el flujo E2E
- [x] Flags para activar/desactivar HPO y usar modelos tuneados

## Guardar y usar el preprocesador (.joblib)

En la √∫ltima secci√≥n del notebook `02_Feature_Engineering_Modelado.ipynb` se incluye una celda para entrenar el preprocesador activo y guardarlo como artefacto reutilizable.

- Qu√© guarda:
    - `models/preprocessor_active_<timestamp>.joblib`: el `ColumnTransformer`/`Pipeline` final ajustado sobre el set de entrenamiento.
    - `models/preprocessor_active_<timestamp>.json`: metadatos (umbral de importancia, tama√±os, fecha, hash de columnas, etc.).

- C√≥mo cargarlo y usarlo en otro script o sesi√≥n:
  
    Ejemplo m√≠nimo en Python:
  
    1) Cargar el artefacto
    2) Aplicar `transform` sobre un DataFrame con el mismo esquema de columnas que el de entrenamiento

    Notas:
    - El artefacto espera las mismas columnas ‚Äúraw‚Äù de entrada que se usaron en el entrenamiento (mismo `df_groups_final`).
    - Si cambian los grupos o el umbral de importancia, se debe volver a entrenar y guardar un nuevo artefacto.

- Versionado y .gitignore:
    - Por defecto, `models/*.joblib` y `models/*.json` est√°n ignorados en `.gitignore` para evitar subir artefactos pesados.
    - Si necesitas versionar un artefacto concreto, puedes:
        - Forzar el agregado con `git add -f models/preprocessor_active_YYYYMMDD_HHMMSS.joblib` y su `.json`, o
        - Quitar/ajustar la regla de `.gitignore` para `models/*`.

Sugerencia: mant√©n un naming consistente y documenta en el metadato el dataset y los flags utilizados (por ejemplo, `IMPORTANCE_THRESHOLD`).

## üìÑ Licencia

MIT (ver archivo LICENSE).
